{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 - MLRun Serving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mlrun serving can take MLRun models or standard model files and produce managed real-time serverless functions based on Nuclio real-time serverless engine, which can be deployed everywhere\n",
    "\n",
    "##### Nuclio is a high-performance \"serverless\" framework focused on data, I/O, and compute intensive workloads. More details can be found on https://github.com/nuclio/nuclio \n",
    "\n",
    "##### Simple model serving classes can be written in Python or be taken from a set of pre-developed ML/DL classes, the code can handle complex data, feature preparation, binary data (images/video). The serving engine supports the full lifecycle including auto generation of micro-services, APIs, load-balancing, logging, model monitoring, configuration management, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /User/new-tutorials/conf\n",
      "Project name: getting-started-tutorial-admin\n"
     ]
    }
   ],
   "source": [
    "from os import path, getenv\n",
    "from mlrun import load_project\n",
    "\n",
    "project_path = path.abspath('conf')\n",
    "project = load_project(project_path)\n",
    "\n",
    "print(f'Project path: {project_path}\\nProject name: {project.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing A Simple Serving Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The class is initialized automatically by the model server , all you need is to implement two mandatory methods:\n",
    "\n",
    "##### load() - download the model file(s) and load the model into memory, note this can be done synchronously or asynchronously\n",
    "##### predict() - accept request payload and return prediction/inference results\n",
    "\n",
    "##### More detailed information on serving classes can be found here : https://github.com/mlrun/mlrun/blob/master/mlrun/serving/README.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is an example of minimal sklearn serving function example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudpickle import load\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import mlrun\n",
    "\n",
    "class ClassifierModel(mlrun.serving.V2ModelServer):\n",
    "    def load(self):\n",
    "        \"\"\"load and initialize the model and/or other elements\"\"\"\n",
    "        model_file, extra_data = self.get_model('.pkl')\n",
    "        self.model = load(open(model_file, 'rb'))\n",
    "\n",
    "    def predict(self, body: dict) -> List:\n",
    "        \"\"\"Generate model predictions from sample.\"\"\"\n",
    "        feats = np.asarray(body['inputs'])\n",
    "        result: np.ndarray = self.model.predict(feats)\n",
    "        return result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Model Serving Function (Service)\n",
    "\n",
    "##### In order to provision a serving function we need to create an MLRun function of type serving , this can be done by using the code_to_function() call from a notebook. We can also import an existing serving function/template from the marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The below code converts the ClassifierModel class sbove to a serving function. The name of the class to be used is set using spec.default_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function\n",
    "serving_fn = code_to_function('mlrun-model', kind='serving',image='mlrun/mlrun')\n",
    "serving_fn.spec.default_class = 'ClassifierModel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add the model created in previous notebook by the training function  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn.add_model('my_model',model_path='store://getting-started-tutorial-admin/train-iris-train_iris_model#b2ec6c1ed096494b88318f01de03f508')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_fn = serving_fn.apply(mlrun.mount_v3io(remote='projects',mount_path='/v3io/projects'))\n",
    "\n",
    "serving_fn = project.set_function(serving_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy the function - this will build and deploy a Nuclio serving function. Once function is deployed successfully, http endpoint will be available which can get inference requests, call the predict method and return the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-12-27 15:31:04,170 [info] Starting remote function deploy\n",
      "2020-12-27 15:31:04  (info) Deploying function\n",
      "2020-12-27 15:31:04  (info) Building\n",
      "2020-12-27 15:31:04  (info) Staging files and preparing base images\n",
      "2020-12-27 15:31:04  (info) Building processor image\n",
      "2020-12-27 15:31:05  (info) Build complete\n",
      "2020-12-27 15:31:11  (info) Function deploy complete\n",
      "> 2020-12-27 15:31:12,110 [info] function deployed, address=default-tenant.app.product-new.iguazio-cd2.com:30845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://default-tenant.app.product-new.iguazio-cd2.com:30845'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serving_fn.deploy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the new Model Serving Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New endpoint was created for the function and can be accessed via http\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The address for the function is http://default-tenant.app.product-new.iguazio-cd2.com:30845 \n",
      "\n",
      "{\"name\": \"ModelRouter\", \"version\": \"v2\", \"extensions\": []}"
     ]
    }
   ],
   "source": [
    "function_address = serving_fn.spec.command\n",
    "print (f'The address for the function is {function_address} \\n')\n",
    "\n",
    "!curl $function_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test server by sending data for inference\n",
    "##### The invoke method enables to programmatically test the function\n",
    "##### For infer, specify the model name followed by infer =>  /v2/models/{model_name}/infer\n",
    "##### For complete model service API commands such as list models, get model health and explain see https://github.com/mlrun/mlrun/blob/master/mlrun/serving/README.md#model-server-api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '01bc5918-be50-4278-b7b7-80749c77baed',\n",
       " 'model_name': 'my_model',\n",
       " 'outputs': [0, 2]}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = '''{\"inputs\":[[5.1, 3.5, 1.4, 0.2],[7.7, 3.8, 6.7, 2.2]]}'''\n",
    "serving_fn.invoke('/v2/models/my_model/infer', my_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Nuclio serving function in UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the projects screen, select the project and then select \"Real-time functions (nuclio)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/nuclio-deploy.png\" alt=\"Jobs\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MlrunProject.save of <mlrun.projects.project.MlrunProject object at 0x7fd1a3618150>>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation! You've completed tutorial 3 of the Iguazio Data Science Platform.\n",
    "Go to [Tutorial 4](tutorial-4.ipynb) to learn how to create an automated pipeline for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
